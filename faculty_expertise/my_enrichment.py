# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_enrichment.ipynb.

# %% auto 0
__all__ = ['client', 'try_parse_json', 'gather_research_links', 'get_corpus_from_urls', 'summarize_faculty_expertise',
           'cache_expertise', 'enrich_faculty_row']

# %% ../nbs/02_enrichment.ipynb 2
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv
from urllib.parse import urljoin, urlparse
import os
import fitz
import json
import re

# %% ../nbs/02_enrichment.ipynb 3
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# %% ../nbs/02_enrichment.ipynb 5
def try_parse_json(raw_text):
    "Cleans up GPT output and returns a parsed JSON object (dict)"
    if not raw_text or not isinstance(raw_text, str):
        return {}

    # Remove Markdown code block fences
    cleaned = re.sub(r'^```(?:json)?', '', raw_text.strip(), flags=re.IGNORECASE).strip()
    cleaned = re.sub(r'```$', '', cleaned).strip()

    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        print("⚠️ JSON decode error:", e)
        print("Offending text (preview):", cleaned[:300])
        return {}

# %% ../nbs/02_enrichment.ipynb 7
def gather_research_links(base_url, max_pages=6):
    """Gathers internal and external URLs relevant to faculty research, skipping Google Scholar fetch."""
    visited = set()
    all_urls = []
    orcid_url = None
    scholar_url = None
    cv_url = None

    try:
        resp = requests.get(base_url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        links = [a['href'] for a in soup.find_all('a', href=True)]

        for href in links:
            full_url = urljoin(base_url, href)
            if full_url in visited:
                continue
            visited.add(full_url)

            if 'scholar.google' in href and not scholar_url:
                scholar_url = full_url
                print(f"Logging Google Scholar link: {full_url}")
            elif 'orcid.org' in href and not orcid_url:
                orcid_url = full_url
            elif full_url.lower().endswith('.pdf') and ('cv' in href.lower() or 'vita' in href.lower()):
                if not cv_url:
                    cv_url = full_url
                all_urls.append(full_url)
            elif urlparse(full_url).netloc == urlparse(base_url).netloc:
                if any(k in href.lower() for k in ['research', 'project', 'publication', 'bio', 'cv', 'about', 'news']):
                    all_urls.append(full_url)

        all_urls = list(set([base_url] + all_urls))[:max_pages]

    except Exception as e:
        print(f"Error gathering links from {base_url}: {e}")

    return {
        "Crawled URLs": all_urls,
        "ORCID URL": orcid_url,
        "Google Scholar URL": scholar_url,
        "CV URL": cv_url
    }



# %% ../nbs/02_enrichment.ipynb 10
def get_corpus_from_urls(urls):
    """Fetches and concatenates cleaned text from a list of URLs, including OCR for PDFs."""
    full_text = ''

    for url in urls:
        try:
            if url.lower().endswith('.pdf'):
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                with open("_temp_cv.pdf", "wb") as f:
                    f.write(response.content)
                doc = fitz.open("_temp_cv.pdf")
                for page in doc:
                    full_text += ' ' + page.get_text()
                doc.close()
                os.remove("_temp_cv.pdf")
            else:
                resp = requests.get(url, timeout=10)
                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, 'html.parser')
                full_text += ' ' + ' '.join(soup.stripped_strings)
        except Exception as e:
            print(f"Failed to fetch {url}: {e}")
            continue

    return full_text.strip()

# %% ../nbs/02_enrichment.ipynb 14
def summarize_faculty_expertise(text, length=750):
    "Return a python dictionary of faculty research specialization using a consistent schema"
    prompt = f"""
You are assisting a university research office in building a structured directory of faculty expertise.

Based on the following faculty webpage content, produce a JSON object with the following fields:

- Research Title: a short title summarizing the faculty’s main research area.
- Expertise: a 1-2 sentence summary of the research focus written for a broad academic audience.
- Research Description: a 1-2 paragraph description of the faculty's research written for a broad audience and suitable for a university website.
- Topics: a list of high-level research themes.
- Methods: a list of research methods or tools used.
- Geographic Focus: a list of countries, regions, or global.
- Keywords: a list of 5–10 freeform keywords.
- Disciplines: a list of academic fields or disciplines.
- Potential Applications: a list of relevant societal, environmental, or economic applications.

Faculty Webpage Text:
{text[:8000]}

Respond only with a JSON object.
"""
    try:
        completion = client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=length,
            temperature=0.3
        )
        return try_parse_json(completion.choices[0].message.content.strip())
    except Exception as e:
        print(f"OpenAI error: {e}")
        return None


# %% ../nbs/02_enrichment.ipynb 19
def cache_expertise(func):
    """Decorator to cache the results of the expertise function. Needs to be able to handle a pd.Series as input """
    cache = {}
    
    def wrapper(row):
        # If row is a pandas Series, we need to check attributes differently
        # than for other types
        if hasattr(row, 'to_dict'):
            # For DataFrame rows or Series, we need a hashable key
            row_key = tuple(row.items())
            if row_key not in cache:
                cache[row_key] = func(row)
            return cache[row_key]
        
        # For simple types like strings
        if row not in cache:
            cache[row] = func(row)
        return cache[row]
    
    return wrapper

@cache_expertise
def enrich_faculty_row(row):
    """Given a row with a Website, returns a dictionary of enriched fields."""
    url = row.get("Website")
    if not url:
        return {}

    metadata = gather_research_links(url)
    corpus = get_corpus_from_urls(metadata["Crawled URLs"])
    summary = summarize_faculty_expertise(corpus)

    return {
        **metadata,
        **summary  # expands structured JSON into flat columns
    }
