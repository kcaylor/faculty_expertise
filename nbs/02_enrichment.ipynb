{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f2e6f2",
   "metadata": {},
   "source": [
    "# Faculty Expertise Enrichment\n",
    "\n",
    "This notebook defines general enrichment logic for scraping additional metadata from faculty websites. Currently, we support:\n",
    "\n",
    "- Fetching raw HTML and text\n",
    "- Generating a scholarly summary using OpenAI's GPT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp my_enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "import fitz\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e465952",
   "metadata": {},
   "source": [
    "### JSON Cleaner\n",
    "\n",
    "A helper function to make sure LLM generated JSON has it's markdown code fence removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def try_parse_json(raw_text):\n",
    "    \"Cleans up GPT output and returns a parsed JSON object (dict)\"\n",
    "    if not raw_text or not isinstance(raw_text, str):\n",
    "        return {}\n",
    "\n",
    "    # Remove Markdown code block fences\n",
    "    cleaned = re.sub(r'^```(?:json)?', '', raw_text.strip(), flags=re.IGNORECASE).strip()\n",
    "    cleaned = re.sub(r'```$', '', cleaned).strip()\n",
    "\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"⚠️ JSON decode error:\", e)\n",
    "        print(\"Offending text (preview):\", cleaned[:300])\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67a42a",
   "metadata": {},
   "source": [
    "## Fetch Faculty/Researcher Content\n",
    "\n",
    "Takes a provided faculty URL, which is typically either a link to a personal website or a link to a departmental website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gather_research_links(base_url, max_pages=6):\n",
    "    \"\"\"Gathers internal and external URLs relevant to faculty research, skipping Google Scholar fetch.\"\"\"\n",
    "    visited = set()\n",
    "    all_urls = []\n",
    "    orcid_url = None\n",
    "    scholar_url = None\n",
    "    cv_url = None\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(base_url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "        for href in links:\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if full_url in visited:\n",
    "                continue\n",
    "            visited.add(full_url)\n",
    "\n",
    "            if 'scholar.google' in href and not scholar_url:\n",
    "                scholar_url = full_url\n",
    "                print(f\"Logging Google Scholar link: {full_url}\")\n",
    "            elif 'orcid.org' in href and not orcid_url:\n",
    "                orcid_url = full_url\n",
    "            elif full_url.lower().endswith('.pdf') and ('cv' in href.lower() or 'vita' in href.lower()):\n",
    "                if not cv_url:\n",
    "                    cv_url = full_url\n",
    "                all_urls.append(full_url)\n",
    "            elif urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "                if any(k in href.lower() for k in ['research', 'project', 'publication', 'bio', 'cv', 'about', 'news']):\n",
    "                    all_urls.append(full_url)\n",
    "\n",
    "        all_urls = list(set([base_url] + all_urls))[:max_pages]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error gathering links from {base_url}: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"Crawled URLs\": all_urls,\n",
    "        \"ORCID URL\": orcid_url,\n",
    "        \"Google Scholar URL\": scholar_url,\n",
    "        \"CV URL\": cv_url\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0963645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Google Scholar link: https://scholar.google.com/citations?user=VGaoB64AAAAJ\n",
      "{'Crawled URLs': ['https://waves.eri.ucsb.edu/publications/', 'https://waves.eri.ucsb.edu/assets/files/KCaylor_CV.pdf', 'https://waves.eri.ucsb.edu'], 'ORCID URL': None, 'Google Scholar URL': 'https://scholar.google.com/citations?user=VGaoB64AAAAJ', 'CV URL': 'https://waves.eri.ucsb.edu/assets/files/KCaylor_CV.pdf'}\n"
     ]
    }
   ],
   "source": [
    "response = gather_research_links(\"https://waves.eri.ucsb.edu\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa804bf8",
   "metadata": {},
   "source": [
    "### Get corpus from URLs\n",
    "\n",
    "Use a list of URLs to develop a corpus of text that can be summarized in a structured manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_corpus_from_urls(urls):\n",
    "    \"\"\"Fetches and concatenates cleaned text from a list of URLs, including OCR for PDFs.\"\"\"\n",
    "    full_text = ''\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            if url.lower().endswith('.pdf'):\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                with open(\"_temp_cv.pdf\", \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                doc = fitz.open(\"_temp_cv.pdf\")\n",
    "                for page in doc:\n",
    "                    full_text += ' ' + page.get_text()\n",
    "                doc.close()\n",
    "                os.remove(\"_temp_cv.pdf\")\n",
    "            else:\n",
    "                resp = requests.get(url, timeout=10)\n",
    "                resp.raise_for_status()\n",
    "                soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "                full_text += ' ' + ' '.join(soup.stripped_strings)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return full_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e81f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_corpus_from_urls(response[\"Crawled URLs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e17f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications from the WAVES Lab - Water, Vegetation, & Society WAVES @ Water, Vegetation, & Society News Our Team Teaching Opportunities Publications CV gScholar Site Archive Toggle Menu Publications from the WAVES Lab Smallholder social networks: Advice seeking and adaptation in rural Kenya Giroux, S. et al. (2023). Smallholder social networks: Advice seeking and adaptation in rural Kenya. Agricultural Systems, doi:10.1016/j.agsy.2022.103574. Modeling seasonal vegetation phenology from hydroclimatic drivers for contrasting plant functional groups within drylands of the Southwestern USA Warter, M. et al. (2023). Modeling seasonal vegetation phenology from hydroclimatic drivers for contrasting plant functional groups within drylands of the Southwestern USA. Environmental Research: Ecology, doi:10.1088/2752-664X/acb9a0. Fluxbots: A Method for Building, Deploying, Collecting and Analyzing Data From an Array of Inexpensive, Autonomous Soil Carbon Flux Chambers Forbes, E. et al. (2023). Flu\n"
     ]
    }
   ],
   "source": [
    "print(result[:1000])  # Print the first 1000 characters of the fetched text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db5aee",
   "metadata": {},
   "source": [
    "## Summarize Faculty Expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarize_faculty_expertise(text, length=750):\n",
    "    \"Return a python dictionary of faculty research specialization using a consistent schema\"\n",
    "    prompt = f\"\"\"\n",
    "You are assisting a university research office in building a structured directory of faculty expertise.\n",
    "\n",
    "Based on the following faculty webpage content, produce a JSON object with the following fields:\n",
    "\n",
    "- Research Title: a short title summarizing the faculty’s main research area.\n",
    "- Expertise: a 1-2 sentence summary of the research focus written for a broad academic audience.\n",
    "- Research Description: a 1-2 paragraph description of the faculty's research written for a broad audience and suitable for a university website.\n",
    "- Topics: a list of high-level research themes.\n",
    "- Methods: a list of research methods or tools used.\n",
    "- Geographic Focus: a list of countries, regions, or global.\n",
    "- Keywords: a list of 5–10 freeform keywords.\n",
    "- Disciplines: a list of academic fields or disciplines.\n",
    "- Potential Applications: a list of relevant societal, environmental, or economic applications.\n",
    "\n",
    "Faculty Webpage Text:\n",
    "{text[:8000]}\n",
    "\n",
    "Respond only with a JSON object.\n",
    "\"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=length,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return try_parse_json(completion.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fae2a",
   "metadata": {},
   "source": [
    "### Summarize expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = summarize_faculty_expertise(result, length=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b2bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Title: Sustainable Agriculture and Environmental Management\n",
      "Expertise: This research focuses on the intersection of water management, agricultural sustainability, and environmental impacts, with a particular emphasis on smallholder farming systems.\n",
      "Research Description: The WAVES Lab conducts multidisciplinary research aimed at understanding and improving the relationships between water, vegetation, and society. By exploring topics such as smallholder social networks, seasonal vegetation phenology, and the impacts of climate variability on agricultural practices, the lab seeks to develop sustainable solutions for water and agricultural management. The research spans from technological innovations like autonomous soil carbon flux chambers to socio-environmental analyses such as the effects of transportation infrastructure on agricultural supply chains. Through a combination of field studies, modeling, and data analysis, the lab aims to address critical challenges facing smallholder farmers and environmental sustainability.\n",
      "Topics: ['Water Management', 'Agricultural Sustainability', 'Environmental Impacts', 'Climate Variability', 'Technological Innovations in Agriculture']\n",
      "Methods: ['Field Studies', 'Modeling', 'Data Analysis', 'Remote Sensing', 'Machine Learning']\n",
      "Geographic Focus: ['Kenya', 'Southwestern USA', 'Zambia', 'Southern California', 'Arizona']\n",
      "Keywords: ['Smallholder Farming', 'Vegetation Phenology', 'Soil Carbon Flux', 'Evapotranspiration', 'Rhizosphere Hydrology', 'Maize Yield', 'Transportation Infrastructure', 'Mobile Phone Data', 'Climate Change', 'Precision Irrigation']\n",
      "Disciplines: ['Agricultural Systems', 'Environmental Research', 'Climatic Change', 'Hydrology', 'Remote Sensing']\n",
      "Potential Applications: ['Improving agricultural productivity', 'Enhancing water resource management', 'Informing climate adaptation strategies', 'Supporting sustainable environmental practices', 'Developing precision agriculture technologies']\n"
     ]
    }
   ],
   "source": [
    "# Print output in a readable format:\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986c0af",
   "metadata": {},
   "source": [
    "## Enrich Faculty Row\n",
    "\n",
    "Function that uses a `pd.Series` (row of dataframe) to enrich with OpenAI summation. Can be used with `pd.DataFrames`, but needs to be checked for API rate limiting. Probably will need to use a threading approach to handle multiple API calls asynchronously.\n",
    "\n",
    "Currently, this is only used in index.ipynb as an example for *a single row*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02fd6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cache_expertise(func):\n",
    "    \"\"\"Decorator to cache the results of the expertise function. Needs to be able to handle a pd.Series as input \"\"\"\n",
    "    cache = {}\n",
    "    \n",
    "    def wrapper(row):\n",
    "        # If row is a pandas Series, we need to check attributes differently\n",
    "        # than for other types\n",
    "        if hasattr(row, 'to_dict'):\n",
    "            # For DataFrame rows or Series, we need a hashable key\n",
    "            row_key = tuple(row.items())\n",
    "            if row_key not in cache:\n",
    "                cache[row_key] = func(row)\n",
    "            return cache[row_key]\n",
    "        \n",
    "        # For simple types like strings\n",
    "        if row not in cache:\n",
    "            cache[row] = func(row)\n",
    "        return cache[row]\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@cache_expertise\n",
    "def enrich_faculty_row(row):\n",
    "    \"\"\"Given a row with a Website, returns a dictionary of enriched fields.\"\"\"\n",
    "    url = row.get(\"Website\")\n",
    "    if not url:\n",
    "        return {}\n",
    "\n",
    "    metadata = gather_research_links(url)\n",
    "    corpus = get_corpus_from_urls(metadata[\"Crawled URLs\"])\n",
    "    summary = summarize_faculty_expertise(corpus)\n",
    "\n",
    "    return {\n",
    "        **metadata,\n",
    "        **summary  # expands structured JSON into flat columns\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
